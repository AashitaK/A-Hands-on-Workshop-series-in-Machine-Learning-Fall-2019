{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Hands-on Workshop series in Machine Learning\n",
    "### Session 2: Decision Trees and Random Forest \n",
    "#### Instructor: Aashita Kesarwani\n",
    "\n",
    "### Decision Trees:\n",
    "\n",
    "Inspired by how we make decisions:\n",
    "* Understand how different variables affect the outcomes, \n",
    "* Consider all the possible scenarios,\n",
    "* Weigh on the pros and cons of our choices. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/0*Yclq0kqMAwCQcIV_.jpg\" width=\"300\"/>\n",
    "\n",
    "Tree is a good tool to visualize how to make decisions in a simple way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can be very useful for classifying examples into labels (<span style=\"color:green\">YES</span>/<span style=\"color:red\">NOPE</span>) given a number of variables (CREDIT HISTORY, HAVE A PLEDGE, HAVE A DEBT, GUARANTORS) to make that decision. \n",
    "\n",
    "<img src=\"https://i.vas3k.ru/7w3.jpg\" width=\"400\"/>\n",
    "\n",
    "The Titanic dataset we worked with earlier is a good case for classification. How would you create a decision tree for it? \n",
    "\n",
    "***Exercise: Sketch a decision tree for the prediction of whether passengers would survive or not given certain features such as age, gender, ticket-class, ticket-fare, etc. The tree need not be the most optimal one and need not include all the features.***\n",
    "\n",
    "The questions to think about:\n",
    "* There can be many different ways to build a decision tree for a particular scenario. How can we generate an effective one? Given two trees, how do we know which one is better? \n",
    "* How can we make use of data to build our decision tree? \n",
    "\n",
    "There are multiple algorithms to build the decision trees, but we will discuss the most commonly used one - CART (Classification And Regression Trees). The algorithm will decide how to split the nodes at each level starting from the top (root).\n",
    "\n",
    "Terminology:\n",
    "* Root\n",
    "* Split nodes\n",
    "* Branches: usually two, but can be more.\n",
    "* Leaf nodes\n",
    "\n",
    "The paths from the root to the leaf nodes represents the classification rules. Each example will fall in exactly one of the leaf nodes.\n",
    "\n",
    "Let the leaf nodes be denoted by $R_m$ for $m = 1 \\dots M$, then the identity function $I_{R_m}$ will be 1 if and only if the example belongs to it, otherwise zero.\n",
    "\n",
    "The predicted output for an example with features $x$ is given by \n",
    "\n",
    "$$ \\hat{y} = f(x) = \\sum_{m=1}^M c_m I_{R_m}(x) $$\n",
    "\n",
    "If an example, say $x$, falls in $R_k$, then $\\hat{y} = c_k$ where $c_k$ is the average of the outputs for all training examples in $R_k$. Clearly, we want $c_k$ to be close to the true output for the example $x$ and also for all the examples falling in that leaf node. This can be achieved if our tree is built such that the outputs for all examples in a node are close to each other. \n",
    "\n",
    "This can be rephrased as: For a good decision tree, each leaf node must consists of the examples belonging to the same class as much as possible.\n",
    "\n",
    "Statistically, this means minimizing the variance of $y$. To achieve this, the splitting measure Gini Index is used to create the decision tree.\n",
    "\n",
    "Let the feature $x$ have the partitions $x_i$. The Gini Index for each partition takes the square of the proportion for each of the examples being classified, finds the sum, then takes the complement of this value.\n",
    "\n",
    "$$Gini(x_i) = 1 - \\sum_{m=1}^M p_{i,m}^2$$\n",
    "\n",
    "where $p_{i,m}$'s are the proportions/probabilities of the examples being classified to $M$ different classes.\n",
    "\n",
    "Note: \n",
    "* If $x$ is a categorical variable, then the partitions $x_i$ are the classes for $x$.\n",
    "* If $x$ is a numerical variable, then the partitions are created for $x$. For example, in the titantic dataset, we can use the feature *Age* to split the node by partitioning it two categories - less than and greater than 18 years.\n",
    "\n",
    "\n",
    "The Gini Index for the feature $x$ is the weighted average of the Gini index for all the partitions $x_i$:\n",
    "\n",
    "$$Gini(x) = \\sum_i \\frac{|x_i|}{|x|} Gini(x_i)$$\n",
    "\n",
    "The formulae will become clear once we do some calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the Titanic example to better understand the Gini Index measure. Support we have derived the following table by counting the number of passengers following in each group.\n",
    "\n",
    "|  Gender | Survived | Died | Total | \n",
    "|-----|----------|------|--------|  \n",
    "|Male | 100 | 400 | 500  |\n",
    "|Female | 80 | 240 | 320 |\n",
    "|Total |  $ $ | $ $ | 820|\n",
    "\n",
    "Remark: For ease in calculation, I have approximated the numbers in the table.\n",
    "\n",
    "***Exercise:*** \n",
    "\n",
    "Calculate $Gini(Gender)$.\n",
    "\n",
    "Hints:   \n",
    "First calculate:\n",
    "* $Gini(Male)$\n",
    "* $Gini(Female)$ \n",
    "\n",
    "Then use: $$Gini(Gender) = \\text{Proportion of Males} * Gini(Male) + \\text{Proportion of Females} * Gini(Female)$$\n",
    "\n",
    "Similarly, the Gini Index for all other features such as age, ticket class, etc. are calculated. The ***feature with the lowest Gini Index*** is chosen to split a node, starting from the root node and then the process is repeated for each branch using conditional probabilities.\n",
    "\n",
    "An example decision tree for the titanic dataset:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would the decision tree be most optimal if it correctly classifies all the labeled examples in our dataset?\n",
    "\n",
    "Three decision trees A, B and C are created using a given labeled dataset. The accuracy of the decision trees in predicting the labels correctly on the same dataset is as follows.\n",
    "\n",
    "|Models | Accuracy| \n",
    "|---|---|\n",
    "| Model A | 100%|\n",
    "| Model B | 85%|\n",
    "| Model C | 70%|\n",
    "\n",
    "Clearly, model A is better at predicting labels for the given dataset than model B and C. Do you think model A will do a better job in predicting labels for yet unseen data as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question, let us consider this binary classification problem. \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/19/Overfitting.svg\" width=\"250\" height=\"250\" />\n",
    "\n",
    "* Which of the two decision boundaries (black or green) will have a lower value for the cost function?\n",
    "* Which decision boundary would you prefer for classifying the unseen examples?\n",
    "\n",
    "\n",
    "***Over-fitting and under-fitting to the training set***  \n",
    "The models can over-train on a dataset, that is they learn the dataset so well that they do not generalize well to the examples outside of that dataset. \n",
    "\n",
    "If we try to fit too complex of a curve as the decision boundary separating the classes and we don't have enough training examples to estimate the parameters for the curve, then we suffer from over-fitting.\n",
    "\n",
    "On the other hand, if we try separating the classes with an over-simplified curve as the decision boundary and we have enough training examples to estimate a curve that would be a better fit, then we suffer from under-fitting. \n",
    "\n",
    "<img src=\"https://vitalflux.com/wp-content/uploads/2015/02/fittings.jpg\" width=\"600\" height=\"800\" />\n",
    "\n",
    "***Model cross-validation***  \n",
    "\n",
    "How do we know whether our model is overfitting or underfitting to the training set?\n",
    "\n",
    "Answer: At the beginning, we save some examples as the validation set and use it to test the performance of the model. \n",
    "\n",
    "|Models | Accuracy on the training set | Accuracy on the validation set | \n",
    "|---|---|---|\n",
    "| Model A | 90%| 70% |\n",
    "| Model B | 80%| 75% |\n",
    "| Model C | 70%| 65% |\n",
    "\n",
    "* With this additional information, can you guess which model will likely perform better for the unseen data?\n",
    "* Which of these three models would you suspect for overfitting to the training data?\n",
    "* Which of these three models would you suspect for underfitting to the training data?\n",
    "\n",
    "#### Key take-aways so far:\n",
    "- Always save some examples from the datasets for testing model performance.\n",
    "- Pay attention to the model performance on the validation set rather than solely on the training set.\n",
    "- Watch out for both under-fitting and over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to keep a check on overfitting?\n",
    "\n",
    "Tuning the hyper-parameters of the trees:\n",
    "* Early-stopping (or pre-pruning):  \n",
    "    Termination criteria:\n",
    "    * Minimum number of examples in a leaf node\n",
    "    * Maximum depth\n",
    "\n",
    "* Post-pruning: The tree grows until it perfectly classifies all training examples and it is pruned using error estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "path = 'data/'\n",
    "df = pd.read_csv(path + 'titanic.csv')\n",
    "\n",
    "# Filling missing values\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median()) \n",
    "\n",
    "# Encoding categorical variable\n",
    "df['Sex'] = df['Sex'].replace({'male': 0, 'female': 1})\n",
    "\n",
    "# Discarding features for simplicity\n",
    "features_to_keep = ['Age', 'Fare', 'Pclass', 'Sex']\n",
    "X = df[features_to_keep]\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting  the dataset into training and validation set using [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from [scikit-learn](https://scikit-learn.org/stable/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the decision tree classifier implementation [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) from [scikit-learn](https://scikit-learn.org/stable/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the decision tree using the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the accuracy of the classifier on both training and validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree classifier on training set: 0.8458083832335329\n",
      "Accuracy of the Decision Tree classifier on validation set: 0.8161434977578476\n"
     ]
    }
   ],
   "source": [
    "train_acc = DT.score(X_train, y_train)\n",
    "print(\"Accuracy of the Decision Tree classifier on training set:\", train_acc)\n",
    "valid_acc = DT.score(X_valid, y_valid)\n",
    "print(\"Accuracy of the Decision Tree classifier on validation set:\", valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Does it seems like the decision tree is overfitting?\n",
    "* Try tuning the hyperparameters, such as `max_depth` in [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Does it makes a difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros and Cons of Decision Trees\n",
    "\n",
    "##### Pros:\n",
    "* Inutitive and easy to interpret and explain  \n",
    "* Visual representation\n",
    "* Non-linear relationships between features do not affect the performance\n",
    "\n",
    "##### Cons:\n",
    "* Prone to overfitting\n",
    "* Highly sensitive to the training data, small changes in the training data can result in very different trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest:\n",
    "\n",
    "Random forest is an ensemble of the decision trees created in the following manner:\n",
    "\n",
    "* Combining multiple decision trees to create a model that performs better than any of the individual decision trees.\n",
    "* Each decision tree independently predicts a label and the final prediction is decided by a majority vote.\n",
    "* The ensemble nullifies the errors in predictions in the individual decision trees.\n",
    "* The key here is that the predictions from the individual decision trees should be uncorrelated to each other, otherwise they will all make the similar errors in their predictions.\n",
    "\n",
    "To ensure the decision trees' predictions (and errors) are uncorrelated to each other, the following two kinds of randomness are introduced.\n",
    "* Bagging (Bootstrap Aggregation): Random sampling with replacement to generate training datasets for each decision tree from the original dataset as we know decision trees are sensitive to the training data.\n",
    "* Feature Randomness: Randomly selecting a subset of features to split the nodes of the trees.\n",
    "\n",
    "Let there be $m$ training examples and $n$ features in our dataset. Each decision tree is generated as follows.\n",
    "1. Choose $m$ examples out of the training set ***with replacement***. This sample will be used for generating the tree.\n",
    "2. For each node, $k<n$ features are chosen at random to split the node. The constant $k$ remains constant for the entire process of generating the forest.\n",
    "3. There is no pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) implementation from [scikit-learn](https://scikit-learn.org/stable/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Random Forest classifier on training set: 0.9640718562874252\n",
      "Accuracy of the Random Forest classifier on validation set:  0.8251121076233184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "train_acc = RF.score(X_train, y_train)\n",
    "print(\"Accuracy of the Random Forest classifier on training set:\", train_acc)\n",
    "valid_acc = RF.score(X_valid, y_valid)\n",
    "print(\"Accuracy of the Random Forest classifier on validation set: \", valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Compare the performance of the Random Forest with the decision trees above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of Gini Index for the above exercise:\n",
    "\n",
    "|  Gender | Survived | Died | Total | \n",
    "|-----|----------|------|--------|  \n",
    "|Male | 100 | 400 | 500  |\n",
    "|Female | 80 | 240 | 320 |\n",
    "|Total |  $ $ | $ $ | 820|\n",
    "\n",
    "Using $Gini(x_i) = 1 - \\sum_{m=1}^M p_{i,m}^2$,\n",
    "$$Gini(Male) = 1 - \\left(\\left(\\frac{100}{500}\\right)^2 + \\left(\\frac{400}{500}\\right)^2 \\right) = 1 - 0.04 - 0.64 = 0.32$$ \n",
    "$$Gini(Female) = 1 - \\left(\\left(\\frac{80}{320}\\right)^2 + \\left(\\frac{240}{320}\\right)^2 \\right) = 1 - 0.0625 - 0.5625 = 0.375$$ \n",
    "\n",
    "Using $Gini(x) = \\sum_i \\frac{|x_i|}{|x|} Gini(x_i)$,\n",
    "$$Gini(Gender) = \\text{Proportion of Males} * Gini(Male) + \\text{Proportion of Females} * Gini(Female) = \\frac{500}{820}* 0.32 + \\frac{320}{820}* 0.375$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgements:\n",
    "The credits for the images used above are as follows.\n",
    "* Image 1: https://becominghuman.ai/understanding-decision-trees-43032111380f\n",
    "* Image 2: https://vas3k.com/blog/machine_learning/\n",
    "* Image 3: https://commons.wikimedia.org/wiki/File:CART_tree_titanic_survivors.png\n",
    "* Image 4: https://commons.wikimedia.org/wiki/File:Overfitting.svg\n",
    "* Image 5: https://vitalflux.com/wp-content/uploads/2015/02/fittings.jpg\n",
    "\n",
    "#### Further reading:\n",
    "* https://web.stanford.edu/~hastie/Papers/ESLII.pdf  for CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
