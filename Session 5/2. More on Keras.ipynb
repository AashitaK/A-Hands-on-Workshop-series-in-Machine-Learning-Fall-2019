{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Hands-on Workshop series in Machine Learning\n",
    "### Session 5: More on Keras\n",
    "#### Instructor: Aashita Kesarwani\n",
    "\n",
    "Now that we know a bit more about neural nets, we will learn how to implement these new ideas with Keras. We will continue to use the Keras python library with TensorFlow backend. Keras provides a user-friendly interface to use TensorFlow and build the models quickly. Both Keras and TensorFlow are robust and powerful python libraries commonly used for deep learning.\n",
    "\n",
    "There are two ways to build models in Keras, sequential and functional. \n",
    "* The sequential API allows us to create layer-by-layer models with multiple inputs and outputs, but is limited in its flexibility. \n",
    "* The functional API allows us to create models where we can connect any layer to any other layer, rather than only the layers immediately adjacent to it. It is useful for more advanced architectures where sequential API no longer suffices.\n",
    "\n",
    "In this primer we will be focusing on the sequential API for ease and clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of reproducibility, we seed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(10)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [Titanic dataset from Kaggle](https://www.kaggle.com/c/titanic), that we already work with in the earlier sessions. Let us first import the data into a pandas dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path = 'data/'\n",
    "df = pd.read_csv(path + 'titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the number of rows and columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess the dataset and extract our input features `X` and the target output `y` from the dataframe `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_Titles(df):\n",
    "    df.Name = df.Name.apply(lambda name: re.findall(\"\\w+[.]\", name)[0].strip())\n",
    "    df = df.rename(columns = {'Name': 'Title'})\n",
    "    df.Title.replace({'Ms.': 'Miss.', 'Mlle.': 'Miss.', 'Dr.': 'Rare', 'Mme.': 'Mr.', 'Major.': 'Rare', 'Lady.': 'Rare', 'Sir.': 'Rare', 'Col.': 'Rare', 'Capt.': 'Rare', 'Countess.': 'Rare', 'Jonkheer.': 'Rare', 'Dona.': 'Rare', 'Don.': 'Rare', 'Rev.': 'Rare'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def fill_Age(df):\n",
    "    df.Age = df.Age.fillna(df.groupby(\"Title\").Age.transform(\"median\"))\n",
    "    return df\n",
    "\n",
    "def get_Group_size(df):\n",
    "    Ticket_counts = df.Ticket.value_counts()\n",
    "    df['Ticket_counts'] = df.Ticket.apply(lambda x: Ticket_counts[x])\n",
    "    df['Family_size'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['Group_size'] = df[['Family_size', 'Ticket_counts']].max(axis=1)\n",
    "    return df\n",
    "\n",
    "def process_features(df):\n",
    "    df.Sex = df.Sex.astype('category', ordered=False).cat.codes\n",
    "    features_to_keep = ['Age', 'Fare', 'Group_size', 'Pclass', 'Sex']\n",
    "    df = df[features_to_keep]\n",
    "    return df\n",
    "\n",
    "def process_data(df):\n",
    "    df = df.copy()\n",
    "    df = get_Titles(df)\n",
    "    df = fill_Age(df)\n",
    "    df = get_Group_size(df)\n",
    "    df = process_features(df)\n",
    "    medianFare = df['Fare'].median()\n",
    "    df['Fare'] = df['Fare'].fillna(medianFare)\n",
    "    return df\n",
    "\n",
    "X = process_data(df)\n",
    "y = df[\"Survived\"].astype('category', ordered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to split the data into training and testing sets. This is for model cross-validation, that is once our network is trained using training data, we want to measure its performance on the unseen validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a simple network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the relevant functions from [Keras](https://keras.io/) that we plan to use in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras import losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a squential, feedforward multilayer perceptron we need to start by constructing an appropriately formatted neural network. Let us first look at the input size, that is the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 5\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1] # size of input variables\n",
    "print(\"Input size:\", input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the network will take in 5 input variables and output a single variable. We will first work out a simple example with one input layer, one hidden layer, and one output layer. Because we are taking in 5 inputs and deterimining one output, we want our first layer to have dimension 5 and our last to have dimension 1.\n",
    "\n",
    "Let us construct a network with a 5-5-1 architecture, meaning that the input layer have 5 nodes, the hidden layers have 5 nodes, and the output layer has a single node for the binary classification - survived or not. \n",
    "\n",
    "We begin by creating a Sequential model using [Keras Sequential API](https://keras.io/getting-started/sequential-model-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by creating a Sequential model, then using the [`.add()`](https://keras.io/getting-started/sequential-model-guide/) function to add the desired layers. For this example we want each layer to be dense, so each node in one layer is connected to every node in the next.\n",
    "\n",
    "Some notes:\n",
    "* We add layers to the model via the [`.add()`](https://keras.io/getting-started/sequential-model-guide/) method. \n",
    "* Keras API allows us to define various kinds of layers that are useful for more involved architectures, but for the Multi-layer Perceptrons, in which every node is connected to every other node in the consequents layers, we use [`Dense`](https://keras.io/layers/core/) layers. \n",
    "* The very first layer added to the model needs to be given the input shape (`input_shape`) or input dimension (`input_dim`).\n",
    "* The other two properties we will specify for the Dense layers are\n",
    "    * Number of units: We will use 5 nodes for the hidden layer\n",
    "    * Activation function: We will use sigmoid activation\n",
    "    \n",
    "Let us add the first fully-connected (dense) layer with the sigmoid activation using [`.add()`](https://keras.io/getting-started/sequential-model-guide/) method with [`Dense`](https://keras.io/layers/core/) layer. This is the hidden layer with 5 nodes (or units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=5, input_dim=input_dim, activation=\"sigmoid\")) # Hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add the output layer. \n",
    "* No need to specify the input size, Keras will automatically take it to be 25 - same as the output size of the previous layer. \n",
    "* We specify the the number of units for this layer to be 1 - to give use the prediction of whether the tumor is benign or malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=1, activation=\"sigmoid\")) # Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the structure of the network using the [`.summary()`](https://keras.io/models/about-keras-models/) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 36\n",
      "Trainable params: 36\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to give us the desired structure. Keep in mind that in practice we want to optimize the number of nodes per hidden layer and number of hidden layers, while in this example the layers were chosen for explanatory purposes.\n",
    "\n",
    "### Training and testing the model \n",
    "\n",
    "Now that we have defined our network architecture, we need to compile it first using [`compile`](https://keras.io/models/model/) before running it. There are a few keywords that we need to pass:\n",
    "* `optimizer`: We will use the stochastic gradient descent menthod `SGD` for the optimizing the cost/loss function, that we studied earlier. We will set the learning rate (step size) to be `lr=0.001`. There are also a few variants of this method that we will learn and use in the next session.\n",
    "* `loss`: This is nothing but the cost function. We want to build a model for binary classification and hence will use cross-entropy loss, as covered in the logistic classifier in the last session.\n",
    "* `metrics`: We studied a lot of classification metrics in our last session. Here, we will simply use accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the model using 10 rounds (epochs) over the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "712/712 [==============================] - 0s 676us/step - loss: 0.7636 - acc: 0.3778\n",
      "Epoch 2/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.7552 - acc: 0.3778\n",
      "Epoch 3/30\n",
      "712/712 [==============================] - 0s 49us/step - loss: 0.7463 - acc: 0.3904\n",
      "Epoch 4/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.7370 - acc: 0.4059\n",
      "Epoch 5/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.7269 - acc: 0.4185\n",
      "Epoch 6/30\n",
      "712/712 [==============================] - 0s 46us/step - loss: 0.7171 - acc: 0.4270\n",
      "Epoch 7/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.7059 - acc: 0.4691\n",
      "Epoch 8/30\n",
      "712/712 [==============================] - 0s 48us/step - loss: 0.6949 - acc: 0.5351\n",
      "Epoch 9/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.6847 - acc: 0.5590\n",
      "Epoch 10/30\n",
      "712/712 [==============================] - 0s 41us/step - loss: 0.6753 - acc: 0.5716\n",
      "Epoch 11/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.6674 - acc: 0.5913\n",
      "Epoch 12/30\n",
      "712/712 [==============================] - 0s 41us/step - loss: 0.6615 - acc: 0.5969\n",
      "Epoch 13/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.6568 - acc: 0.6110\n",
      "Epoch 14/30\n",
      "712/712 [==============================] - 0s 45us/step - loss: 0.6529 - acc: 0.6152\n",
      "Epoch 15/30\n",
      "712/712 [==============================] - 0s 41us/step - loss: 0.6491 - acc: 0.6264\n",
      "Epoch 16/30\n",
      "712/712 [==============================] - 0s 44us/step - loss: 0.6464 - acc: 0.6222\n",
      "Epoch 17/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.6440 - acc: 0.6306\n",
      "Epoch 18/30\n",
      "712/712 [==============================] - 0s 41us/step - loss: 0.6418 - acc: 0.6334\n",
      "Epoch 19/30\n",
      "712/712 [==============================] - 0s 48us/step - loss: 0.6398 - acc: 0.6306\n",
      "Epoch 20/30\n",
      "712/712 [==============================] - 0s 45us/step - loss: 0.6380 - acc: 0.6334\n",
      "Epoch 21/30\n",
      "712/712 [==============================] - 0s 45us/step - loss: 0.6361 - acc: 0.6390\n",
      "Epoch 22/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.6345 - acc: 0.6433\n",
      "Epoch 23/30\n",
      "712/712 [==============================] - 0s 47us/step - loss: 0.6329 - acc: 0.6461\n",
      "Epoch 24/30\n",
      "712/712 [==============================] - 0s 49us/step - loss: 0.6314 - acc: 0.6545\n",
      "Epoch 25/30\n",
      "712/712 [==============================] - 0s 41us/step - loss: 0.6301 - acc: 0.6573\n",
      "Epoch 26/30\n",
      "712/712 [==============================] - 0s 46us/step - loss: 0.6290 - acc: 0.6615\n",
      "Epoch 27/30\n",
      "712/712 [==============================] - 0s 38us/step - loss: 0.6278 - acc: 0.6643\n",
      "Epoch 28/30\n",
      "712/712 [==============================] - 0s 51us/step - loss: 0.6267 - acc: 0.6657\n",
      "Epoch 29/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.6256 - acc: 0.6699\n",
      "Epoch 30/30\n",
      "712/712 [==============================] - 0s 46us/step - loss: 0.6248 - acc: 0.6699\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=30, verbose=1); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that for each epoch the accuracy is generally increasing and the loss is generally decreasing. We now want to see how well it works on our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on testing set...\n",
      "179/179 [==============================] - 0s 510us/step\n",
      "[INFO] loss=0.6479, accuracy: 62.5698%\n"
     ]
    }
   ],
   "source": [
    "# show the accuracy on the testing set\n",
    "print(\"Evaluating on testing set...\")\n",
    "(loss, accuracy) = model.evaluate(X_valid, y_valid, batch_size=5, verbose=1)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what happens if we add an additional hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "712/712 [==============================] - 0s 666us/step - loss: 0.6684 - acc: 0.6222\n",
      "Epoch 2/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.6682 - acc: 0.6222\n",
      "Epoch 3/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.6681 - acc: 0.6222\n",
      "Epoch 4/30\n",
      "712/712 [==============================] - 0s 40us/step - loss: 0.6680 - acc: 0.6222\n",
      "Epoch 5/30\n",
      "712/712 [==============================] - 0s 41us/step - loss: 0.6678 - acc: 0.6222\n",
      "Epoch 6/30\n",
      "712/712 [==============================] - 0s 40us/step - loss: 0.6677 - acc: 0.6222\n",
      "Epoch 7/30\n",
      "712/712 [==============================] - 0s 40us/step - loss: 0.6676 - acc: 0.6222\n",
      "Epoch 8/30\n",
      "712/712 [==============================] - 0s 39us/step - loss: 0.6675 - acc: 0.6222\n",
      "Epoch 9/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.6673 - acc: 0.6222\n",
      "Epoch 10/30\n",
      "712/712 [==============================] - 0s 40us/step - loss: 0.6672 - acc: 0.6222\n",
      "Epoch 11/30\n",
      "712/712 [==============================] - 0s 41us/step - loss: 0.6671 - acc: 0.6222\n",
      "Epoch 12/30\n",
      "712/712 [==============================] - 0s 39us/step - loss: 0.6670 - acc: 0.6222\n",
      "Epoch 13/30\n",
      "712/712 [==============================] - 0s 40us/step - loss: 0.6669 - acc: 0.6222\n",
      "Epoch 14/30\n",
      "712/712 [==============================] - 0s 44us/step - loss: 0.6667 - acc: 0.6222\n",
      "Epoch 15/30\n",
      "712/712 [==============================] - 0s 39us/step - loss: 0.6666 - acc: 0.6222\n",
      "Epoch 16/30\n",
      "712/712 [==============================] - 0s 39us/step - loss: 0.6665 - acc: 0.6222\n",
      "Epoch 17/30\n",
      "712/712 [==============================] - 0s 39us/step - loss: 0.6664 - acc: 0.6222\n",
      "Epoch 18/30\n",
      "712/712 [==============================] - 0s 47us/step - loss: 0.6663 - acc: 0.6222\n",
      "Epoch 19/30\n",
      "712/712 [==============================] - 0s 41us/step - loss: 0.6662 - acc: 0.6222\n",
      "Epoch 20/30\n",
      "712/712 [==============================] - 0s 40us/step - loss: 0.6661 - acc: 0.6222\n",
      "Epoch 21/30\n",
      "712/712 [==============================] - 0s 40us/step - loss: 0.6660 - acc: 0.6222\n",
      "Epoch 22/30\n",
      "712/712 [==============================] - 0s 38us/step - loss: 0.6659 - acc: 0.6222\n",
      "Epoch 23/30\n",
      "712/712 [==============================] - 0s 44us/step - loss: 0.6658 - acc: 0.6222\n",
      "Epoch 24/30\n",
      "712/712 [==============================] - 0s 41us/step - loss: 0.6657 - acc: 0.6222\n",
      "Epoch 25/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.6655 - acc: 0.6222\n",
      "Epoch 26/30\n",
      "712/712 [==============================] - 0s 39us/step - loss: 0.6654 - acc: 0.6222\n",
      "Epoch 27/30\n",
      "712/712 [==============================] - 0s 44us/step - loss: 0.6653 - acc: 0.6222\n",
      "Epoch 28/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.6653 - acc: 0.6222\n",
      "Epoch 29/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.6652 - acc: 0.6222\n",
      "Epoch 30/30\n",
      "712/712 [==============================] - 0s 45us/step - loss: 0.6651 - acc: 0.6222\n",
      "\n",
      "\n",
      "Evaluating on testing set...\n",
      "179/179 [==============================] - 0s 568us/step\n",
      "[INFO] loss=0.6758, accuracy: 59.2179%\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(units=5, input_dim=input_dim, activation=\"sigmoid\")) # 1st hidden layer\n",
    "model2.add(Dense(units=8, activation=\"sigmoid\")) # 2nd hidden layer\n",
    "model2.add(Dense(units=1, activation=\"sigmoid\")) # output layer\n",
    "model2.compile(optimizer=SGD(lr=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model2.fit(X_train, y_train, epochs=30, verbose=1); \n",
    "\n",
    "# show the accuracy on the testing set\n",
    "print(\"\\n\\nEvaluating on testing set...\")\n",
    "(loss, accuracy) = model2.evaluate(X_valid, y_valid, batch_size=5, verbose=1)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function in the above network was not changing much and there was no change in the accuracy metric. It seems that the network was suffering from the vanishing gradient problem as we added more depth to our network, the weight updates were neglible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ReLU as an activation function:\n",
    "\n",
    "To address the vanishing gradient in the above network, we replace the sigmoid activation function with ReLU, expect for the output layer. We also increased the learning rate in the gradient descent by tha factor of 10 by passing `SGD(lr=0.01)` instead of `SGD(lr=0.001)` for the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "712/712 [==============================] - 1s 711us/step - loss: 0.6928 - acc: 0.6236\n",
      "Epoch 2/30\n",
      "712/712 [==============================] - 0s 47us/step - loss: 0.6294 - acc: 0.6222\n",
      "Epoch 3/30\n",
      "712/712 [==============================] - 0s 42us/step - loss: 0.6071 - acc: 0.6222\n",
      "Epoch 4/30\n",
      "712/712 [==============================] - 0s 44us/step - loss: 0.5980 - acc: 0.6320\n",
      "Epoch 5/30\n",
      "712/712 [==============================] - 0s 45us/step - loss: 0.5888 - acc: 0.6826\n",
      "Epoch 6/30\n",
      "712/712 [==============================] - 0s 44us/step - loss: 0.5876 - acc: 0.7065\n",
      "Epoch 7/30\n",
      "712/712 [==============================] - 0s 44us/step - loss: 0.5829 - acc: 0.7205\n",
      "Epoch 8/30\n",
      "712/712 [==============================] - 0s 47us/step - loss: 0.5833 - acc: 0.7107\n",
      "Epoch 9/30\n",
      "712/712 [==============================] - 0s 46us/step - loss: 0.5824 - acc: 0.7177\n",
      "Epoch 10/30\n",
      "712/712 [==============================] - 0s 45us/step - loss: 0.5808 - acc: 0.7247\n",
      "Epoch 11/30\n",
      "712/712 [==============================] - 0s 45us/step - loss: 0.5774 - acc: 0.7177\n",
      "Epoch 12/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.5754 - acc: 0.7135\n",
      "Epoch 13/30\n",
      "712/712 [==============================] - 0s 44us/step - loss: 0.5774 - acc: 0.7219\n",
      "Epoch 14/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.5766 - acc: 0.7177\n",
      "Epoch 15/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.5763 - acc: 0.7247\n",
      "Epoch 16/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.5734 - acc: 0.7233\n",
      "Epoch 17/30\n",
      "712/712 [==============================] - 0s 51us/step - loss: 0.5695 - acc: 0.7191\n",
      "Epoch 18/30\n",
      "712/712 [==============================] - 0s 44us/step - loss: 0.5658 - acc: 0.7247\n",
      "Epoch 19/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.5644 - acc: 0.7219\n",
      "Epoch 20/30\n",
      "712/712 [==============================] - 0s 41us/step - loss: 0.5723 - acc: 0.7331\n",
      "Epoch 21/30\n",
      "712/712 [==============================] - 0s 45us/step - loss: 0.5650 - acc: 0.7261\n",
      "Epoch 22/30\n",
      "712/712 [==============================] - 0s 41us/step - loss: 0.5655 - acc: 0.7163\n",
      "Epoch 23/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.5643 - acc: 0.7233\n",
      "Epoch 24/30\n",
      "712/712 [==============================] - 0s 44us/step - loss: 0.5664 - acc: 0.7219\n",
      "Epoch 25/30\n",
      "712/712 [==============================] - 0s 44us/step - loss: 0.5616 - acc: 0.7317\n",
      "Epoch 26/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.5670 - acc: 0.7135\n",
      "Epoch 27/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.5594 - acc: 0.7233\n",
      "Epoch 28/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.5605 - acc: 0.7261\n",
      "Epoch 29/30\n",
      "712/712 [==============================] - 0s 45us/step - loss: 0.5572 - acc: 0.7275\n",
      "Epoch 30/30\n",
      "712/712 [==============================] - 0s 43us/step - loss: 0.5676 - acc: 0.7275\n",
      "\n",
      "\n",
      "Evaluating on testing set...\n",
      "179/179 [==============================] - 0s 623us/step\n",
      "[INFO] loss=0.5952, accuracy: 67.5978%\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(units=5, input_dim=input_dim, activation=\"relu\")) \n",
    "model3.add(Dense(units=8, activation=\"relu\"))\n",
    "model3.add(Dense(units=1, activation=\"sigmoid\")) \n",
    "model3.compile(optimizer=SGD(lr=0.01), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model3.fit(X_train, y_train, epochs=30, verbose=1); \n",
    "\n",
    "# show the accuracy on the testing set\n",
    "print(\"\\n\\nEvaluating on testing set...\")\n",
    "(loss, accuracy) = model3.evaluate(X_valid, y_valid, batch_size=5, verbose=1)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has indeed improved the network quite significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we increase the complexity of the network by adding more nodes in each layer. We also use the RMSprop as the optimizer instead of gradient descent, as it often performs better for more complex networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "712/712 [==============================] - 1s 904us/step - loss: 1.0509 - acc: 0.6222\n",
      "Epoch 2/100\n",
      "712/712 [==============================] - 0s 53us/step - loss: 0.9386 - acc: 0.6222\n",
      "Epoch 3/100\n",
      "712/712 [==============================] - 0s 53us/step - loss: 0.8769 - acc: 0.6222\n",
      "Epoch 4/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.8385 - acc: 0.6924\n",
      "Epoch 5/100\n",
      "712/712 [==============================] - 0s 53us/step - loss: 0.8115 - acc: 0.6980\n",
      "Epoch 6/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.7869 - acc: 0.6938\n",
      "Epoch 7/100\n",
      "712/712 [==============================] - 0s 54us/step - loss: 0.7660 - acc: 0.6840\n",
      "Epoch 8/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.7498 - acc: 0.6910\n",
      "Epoch 9/100\n",
      "712/712 [==============================] - 0s 73us/step - loss: 0.7361 - acc: 0.6966\n",
      "Epoch 10/100\n",
      "712/712 [==============================] - 0s 60us/step - loss: 0.7240 - acc: 0.6966\n",
      "Epoch 11/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.7148 - acc: 0.6952\n",
      "Epoch 12/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.7042 - acc: 0.6966\n",
      "Epoch 13/100\n",
      "712/712 [==============================] - 0s 57us/step - loss: 0.6940 - acc: 0.6952\n",
      "Epoch 14/100\n",
      "712/712 [==============================] - 0s 54us/step - loss: 0.6869 - acc: 0.7037\n",
      "Epoch 15/100\n",
      "712/712 [==============================] - 0s 54us/step - loss: 0.6808 - acc: 0.7079\n",
      "Epoch 16/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.6767 - acc: 0.7107\n",
      "Epoch 17/100\n",
      "712/712 [==============================] - 0s 53us/step - loss: 0.6723 - acc: 0.7107\n",
      "Epoch 18/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.6682 - acc: 0.7121\n",
      "Epoch 19/100\n",
      "712/712 [==============================] - 0s 58us/step - loss: 0.6634 - acc: 0.7135\n",
      "Epoch 20/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.6598 - acc: 0.7177\n",
      "Epoch 21/100\n",
      "712/712 [==============================] - 0s 49us/step - loss: 0.6539 - acc: 0.7205\n",
      "Epoch 22/100\n",
      "712/712 [==============================] - 0s 57us/step - loss: 0.6481 - acc: 0.7219\n",
      "Epoch 23/100\n",
      "712/712 [==============================] - 0s 52us/step - loss: 0.6433 - acc: 0.7261\n",
      "Epoch 24/100\n",
      "712/712 [==============================] - 0s 57us/step - loss: 0.6372 - acc: 0.7317\n",
      "Epoch 25/100\n",
      "712/712 [==============================] - 0s 60us/step - loss: 0.6301 - acc: 0.7317\n",
      "Epoch 26/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.6231 - acc: 0.7360\n",
      "Epoch 27/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.6174 - acc: 0.7331\n",
      "Epoch 28/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.6124 - acc: 0.7402\n",
      "Epoch 29/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.6062 - acc: 0.7388\n",
      "Epoch 30/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.5986 - acc: 0.7430\n",
      "Epoch 31/100\n",
      "712/712 [==============================] - 0s 54us/step - loss: 0.5918 - acc: 0.7430\n",
      "Epoch 32/100\n",
      "712/712 [==============================] - 0s 58us/step - loss: 0.5854 - acc: 0.7458\n",
      "Epoch 33/100\n",
      "712/712 [==============================] - 0s 59us/step - loss: 0.5768 - acc: 0.7472\n",
      "Epoch 34/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.5700 - acc: 0.7444\n",
      "Epoch 35/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.5653 - acc: 0.7514\n",
      "Epoch 36/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.5583 - acc: 0.7626\n",
      "Epoch 37/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.5535 - acc: 0.7654\n",
      "Epoch 38/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.5481 - acc: 0.7739\n",
      "Epoch 39/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.5422 - acc: 0.7823\n",
      "Epoch 40/100\n",
      "712/712 [==============================] - 0s 54us/step - loss: 0.5385 - acc: 0.7837\n",
      "Epoch 41/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.5344 - acc: 0.7963\n",
      "Epoch 42/100\n",
      "712/712 [==============================] - 0s 58us/step - loss: 0.5313 - acc: 0.7921\n",
      "Epoch 43/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.5284 - acc: 0.8034\n",
      "Epoch 44/100\n",
      "712/712 [==============================] - 0s 57us/step - loss: 0.5260 - acc: 0.8020\n",
      "Epoch 45/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.5222 - acc: 0.8090\n",
      "Epoch 46/100\n",
      "712/712 [==============================] - 0s 57us/step - loss: 0.5206 - acc: 0.8048\n",
      "Epoch 47/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.5181 - acc: 0.8062\n",
      "Epoch 48/100\n",
      "712/712 [==============================] - 0s 57us/step - loss: 0.5160 - acc: 0.8034\n",
      "Epoch 49/100\n",
      "712/712 [==============================] - 0s 58us/step - loss: 0.5125 - acc: 0.8006\n",
      "Epoch 50/100\n",
      "712/712 [==============================] - 0s 53us/step - loss: 0.5126 - acc: 0.8048\n",
      "Epoch 51/100\n",
      "712/712 [==============================] - 0s 57us/step - loss: 0.5092 - acc: 0.8090\n",
      "Epoch 52/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.5081 - acc: 0.8048\n",
      "Epoch 53/100\n",
      "712/712 [==============================] - 0s 58us/step - loss: 0.5047 - acc: 0.8062\n",
      "Epoch 54/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.5029 - acc: 0.8062\n",
      "Epoch 55/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.4977 - acc: 0.8090\n",
      "Epoch 56/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.4955 - acc: 0.7992\n",
      "Epoch 57/100\n",
      "712/712 [==============================] - 0s 57us/step - loss: 0.4931 - acc: 0.8006\n",
      "Epoch 58/100\n",
      "712/712 [==============================] - 0s 52us/step - loss: 0.4910 - acc: 0.7992\n",
      "Epoch 59/100\n",
      "712/712 [==============================] - 0s 53us/step - loss: 0.4928 - acc: 0.7963\n",
      "Epoch 60/100\n",
      "712/712 [==============================] - 0s 57us/step - loss: 0.4871 - acc: 0.8006\n",
      "Epoch 61/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.4864 - acc: 0.8076\n",
      "Epoch 62/100\n",
      "712/712 [==============================] - 0s 67us/step - loss: 0.4833 - acc: 0.8006\n",
      "Epoch 63/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.4829 - acc: 0.8104\n",
      "Epoch 64/100\n",
      "712/712 [==============================] - 0s 60us/step - loss: 0.4837 - acc: 0.7921\n",
      "Epoch 65/100\n",
      "712/712 [==============================] - 0s 58us/step - loss: 0.4812 - acc: 0.7992\n",
      "Epoch 66/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.4780 - acc: 0.8062\n",
      "Epoch 67/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.4758 - acc: 0.8090\n",
      "Epoch 68/100\n",
      "712/712 [==============================] - 0s 57us/step - loss: 0.4781 - acc: 0.8174\n",
      "Epoch 69/100\n",
      "712/712 [==============================] - 0s 58us/step - loss: 0.4743 - acc: 0.8188\n",
      "Epoch 70/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.4748 - acc: 0.8104\n",
      "Epoch 71/100\n",
      "712/712 [==============================] - 0s 59us/step - loss: 0.4701 - acc: 0.8188\n",
      "Epoch 72/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.4706 - acc: 0.8132\n",
      "Epoch 73/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.4664 - acc: 0.8216\n",
      "Epoch 74/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.4672 - acc: 0.8146\n",
      "Epoch 75/100\n",
      "712/712 [==============================] - 0s 57us/step - loss: 0.4646 - acc: 0.8244\n",
      "Epoch 76/100\n",
      "712/712 [==============================] - 0s 54us/step - loss: 0.4635 - acc: 0.8216\n",
      "Epoch 77/100\n",
      "712/712 [==============================] - 0s 52us/step - loss: 0.4618 - acc: 0.8301\n",
      "Epoch 78/100\n",
      "712/712 [==============================] - 0s 54us/step - loss: 0.4595 - acc: 0.8230\n",
      "Epoch 79/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.4590 - acc: 0.8287\n",
      "Epoch 80/100\n",
      "712/712 [==============================] - 0s 61us/step - loss: 0.4597 - acc: 0.8202\n",
      "Epoch 81/100\n",
      "712/712 [==============================] - 0s 54us/step - loss: 0.4567 - acc: 0.8216\n",
      "Epoch 82/100\n",
      "712/712 [==============================] - 0s 60us/step - loss: 0.4551 - acc: 0.8188\n",
      "Epoch 83/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.4519 - acc: 0.8188\n",
      "Epoch 84/100\n",
      "712/712 [==============================] - 0s 52us/step - loss: 0.4565 - acc: 0.8216\n",
      "Epoch 85/100\n",
      "712/712 [==============================] - 0s 61us/step - loss: 0.4521 - acc: 0.8244\n",
      "Epoch 86/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.4510 - acc: 0.8244\n",
      "Epoch 87/100\n",
      "712/712 [==============================] - 0s 52us/step - loss: 0.4475 - acc: 0.8216\n",
      "Epoch 88/100\n",
      "712/712 [==============================] - 0s 54us/step - loss: 0.4471 - acc: 0.8258\n",
      "Epoch 89/100\n",
      "712/712 [==============================] - 0s 50us/step - loss: 0.4484 - acc: 0.8272\n",
      "Epoch 90/100\n",
      "712/712 [==============================] - 0s 56us/step - loss: 0.4443 - acc: 0.8272\n",
      "Epoch 91/100\n",
      "712/712 [==============================] - 0s 54us/step - loss: 0.4454 - acc: 0.8230\n",
      "Epoch 92/100\n",
      "712/712 [==============================] - 0s 54us/step - loss: 0.4432 - acc: 0.8357\n",
      "Epoch 93/100\n",
      "712/712 [==============================] - 0s 51us/step - loss: 0.4448 - acc: 0.8272\n",
      "Epoch 94/100\n",
      "712/712 [==============================] - 0s 53us/step - loss: 0.4416 - acc: 0.8244\n",
      "Epoch 95/100\n",
      "712/712 [==============================] - 0s 53us/step - loss: 0.4423 - acc: 0.8287\n",
      "Epoch 96/100\n",
      "712/712 [==============================] - 0s 58us/step - loss: 0.4399 - acc: 0.8315\n",
      "Epoch 97/100\n",
      "712/712 [==============================] - 0s 58us/step - loss: 0.4407 - acc: 0.8202\n",
      "Epoch 98/100\n",
      "712/712 [==============================] - 0s 60us/step - loss: 0.4372 - acc: 0.8343\n",
      "Epoch 99/100\n",
      "712/712 [==============================] - 0s 53us/step - loss: 0.4363 - acc: 0.8287\n",
      "Epoch 100/100\n",
      "712/712 [==============================] - 0s 55us/step - loss: 0.4364 - acc: 0.8287\n",
      "\n",
      "\n",
      "Evaluating on testing set...\n",
      "179/179 [==============================] - 0s 827us/step\n",
      "[INFO] loss=0.5055, accuracy: 79.3296%\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(units=25, input_dim=input_dim, activation=\"sigmoid\", \n",
    "                 activity_regularizer=regularizers.l2(0.001))) \n",
    "model4.add(Dense(units=10, activation=\"relu\",\n",
    "                activity_regularizer=regularizers.l2(0.001))) \n",
    "model4.add(Dense(units=1, activation=\"sigmoid\")) \n",
    "model4.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model4.fit(X_train, y_train, epochs=100, verbose=1); \n",
    "\n",
    "# show the accuracy on the testing set\n",
    "print(\"\\n\\nEvaluating on testing set...\")\n",
    "(loss, accuracy) = model4.evaluate(X_valid, y_valid, batch_size=5, verbose=1)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The training example is too small to meaningfully learn the tuning of neural networks, and the primer is intended to give you a simple illustration for using Keras in your projects.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgment:\n",
    "* The dataset used in this project is the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
